{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Simple Decision Tree Using CART Algorithm\n",
    "There are several types of decision trees, here I will build a tree from scratch using the CART algorithm. CART stands for Classification And Regression Tree. I first read this algorithm on [this blog](https://machinelearningmastery.com/implement-decision-tree-algorithm-scratch-python/), and determined to try it by myself. I basically followed the same idea, but instead of using dictionaries to store nodes, I will use class to define a node as well as the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a tree node\n",
    "The decision tree is made of several levels of tree nodes, and each node has 2 childs. For leaf nodes, there will be a terminal value that returns as the prediction of classification or regression. If a node only contains one label, then it will automatically becames a leaf node. For other intermediate nodes, there will be index and value, which specify which feature column to compare and the cutoff value for splitting. The index and value are selected by looping over all possible indexes and values in training set, and find the best split that gives the lowest gini index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    def __init__(self, groups):\n",
    "        '''Initialized a node, it has left child and right child, cutoff value,\n",
    "        index for feature column, and terminal value if it's a leaf node.\n",
    "        If a node only has one label, make it a leaf node.\n",
    "        \n",
    "        Input:\n",
    "            groups: a list of examples\n",
    "        '''\n",
    "        self.groups = groups\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.value = None\n",
    "        self.index = None\n",
    "        self.terminal = None\n",
    "        if len(Node.get_labels(groups)) == 1:\n",
    "            self.to_terminal()\n",
    "    def get_size(self):\n",
    "        '''Return the size of examples in current node'''\n",
    "        return len(self.groups)\n",
    "    @staticmethod\n",
    "    def get_labels(groups):\n",
    "        '''Return a list of unique labels'''\n",
    "        return list(set([item[-1] for item in groups]))\n",
    "    @staticmethod\n",
    "    def gini(groups):\n",
    "        '''Calculate the gini index of current node\n",
    "        gini = 1-sum(p(i)**2), i in all labels\n",
    "        '''\n",
    "        if not groups:\n",
    "            return 0\n",
    "        gini = 1\n",
    "        for label in Node.get_labels(groups):\n",
    "            gini -= ([item[-1] for item in groups].count(label)/len(groups))**2\n",
    "        return gini\n",
    "    def split(self):\n",
    "        '''Try different splits and pick the split with the smallest gini index'''\n",
    "        gini = 1\n",
    "        # O(n^2), can we do better?\n",
    "        for index in range(len(self.groups[0])-1):\n",
    "            for value in [item[index] for item in self.groups]:\n",
    "                leftlist, rightlist = self.split_groups(index, value)\n",
    "                gini_c = (len(leftlist)*Node.gini(leftlist)+len(rightlist)*Node.gini(rightlist))/len(self.groups)\n",
    "                if gini_c < gini:\n",
    "                    gini = gini_c\n",
    "                    self.value = value\n",
    "                    self.index = index\n",
    "                    # this may creates a lot of unused Node objects\n",
    "                    self.left = Node(leftlist)\n",
    "                    self.right = Node(rightlist)\n",
    "    def split_groups(self, index, value):\n",
    "        '''Split the group by the index and value\n",
    "        \n",
    "        Input:\n",
    "            index: feature column\n",
    "            value: cutoff value; example with feature column smaller than this will\n",
    "        assigned to left child, otherwise right child.\n",
    "        \n",
    "        Return:\n",
    "            list of examples for left child and right child\n",
    "        '''\n",
    "        leftlist, rightlist = [], []\n",
    "        for example in self.groups:\n",
    "            if example[index] < value:\n",
    "                leftlist.append(example)\n",
    "            else:\n",
    "                rightlist.append(example)\n",
    "        return leftlist, rightlist\n",
    "    def to_terminal(self):\n",
    "        '''Save this node as terminal leaf\n",
    "        '''\n",
    "        labels = Node.get_labels(self.groups)\n",
    "        # might have a problem when the counts are the same\n",
    "        self.terminal = max(labels, key = labels.count)\n",
    "    def get_ind_val(self):\n",
    "        '''Return the index and value'''\n",
    "        return self.index, self.value\n",
    "    def get_terminal(self):\n",
    "        '''Return the classification result'''\n",
    "        return self.terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Will some of the leaves have no terminal values?\n",
    "When I first implemented the code in the previous cell, I was somehow afraid that some leaf nodes won't have terminal values, and may return None if we want to retrieve the terminal value. After a second thought, I now know that it won't happen as long as the min_size is set to 1 or higher. Take the binary case for example. \n",
    "If a node has only one group, then it's a leaf node and won't split again, and the terminal value is just the label of the group.\n",
    "\n",
    "If a parent node has n (n &ge; 2) groups, then it must contains mixed labels, otherwise it's a leaf node and won't have childs. And we will prove that both of its childs are not be None. That's because if one of its childs is None, then the gini index of its two childs is the same with the parent node. Intuitively, the gini index can be reduced by just seperating one positive group from the groups. And this could be proven by following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Let n be the total number of groups in current node, and x be the positive labels (1 &le; x < n).*\n",
    "\n",
    "*The gini index of current node: $gini_0 = 1-\\big(\\frac{x}{n}\\big)^2+\\big(\\frac{n-x}{n}\\big)^2$*\n",
    "\n",
    "*Split one positive label, and the gini index now is $gini_1 = \\frac{n-1}{n}\\bigg(1-\\big(\\frac{x-1}{n-1}\\big)^2+\\big(\\frac{n-x}{n-1}\\big)^2\\bigg)$*\n",
    "\n",
    "*Substract the two equations and after several steps of algebra, we will have:\n",
    "$gini_0-gini_1 = \\frac{2(n-x)^2}{n^2(n-1)} > 0$, which means spliting will be performed and the children will have at least one group.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine these 2 cases together, we know that the leaf node will have at least one group, hence will have a terminal value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a decision tree\n",
    "A decision tree has a root node, and by training the dataset, we will add nodes to it childs. This process is done by recursively splitting the training sets, until the tree reaches the maximum depth, or all its childs are leaf node.\n",
    "\n",
    "For prediction, it's similar. Recursively assigning the camparison of features to nodes from top to bottom until it finds a leaf node and return a terminal value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CART_tree(object):\n",
    "    def __init__(self):\n",
    "        '''Initialize a tree with root node, max_depth and min_size'''\n",
    "        self.root = None\n",
    "        self.max_depth = None\n",
    "        self.min_size = None\n",
    "    def train(self, training, max_depth = 1, min_size = 1):\n",
    "        '''Train the tree with training examples\n",
    "        \n",
    "        Input:\n",
    "            max_depth, int\n",
    "            min_size, int\n",
    "        '''\n",
    "        self.root = Node(training)\n",
    "        self.max_depth = max_depth\n",
    "        self.min_size = min_size\n",
    "        self.rec_split(self.root, 0)\n",
    "    def rec_split(self, node, depth):\n",
    "        '''Recursively split the node, until meet either of the following conditions:\n",
    "        1. Max tree depth or minimum node size reached.\n",
    "        2. The node is leaf node that only has one label.\n",
    "        \n",
    "        Input:\n",
    "            node: Node object\n",
    "            depth: int\n",
    "        '''\n",
    "        if depth == self.max_depth or node.get_size() <= self.min_size:\n",
    "            # reach the max_depth or min_size, return\n",
    "            node.to_terminal()\n",
    "            return\n",
    "        if node.get_terminal() is not None:\n",
    "            # leaf node that only contains one label, return\n",
    "            return\n",
    "        # after previous 2 cases, the remaining nodes have group size larger than 2\n",
    "        # and have mixed labels\n",
    "        node.split()\n",
    "        self.rec_split(node.left, depth+1)\n",
    "        self.rec_split(node.right, depth+1)\n",
    "        return\n",
    "    def predict(self, groups):\n",
    "        '''Given a list of groups, predict their labels\n",
    "        \n",
    "        Input:\n",
    "            groups: list, input features\n",
    "        Return:\n",
    "            preds: list, predicted labels\n",
    "        '''\n",
    "        preds = []\n",
    "        for group in groups:\n",
    "            preds.append(CART_tree._predict(group, self.root))\n",
    "        return preds\n",
    "    @staticmethod\n",
    "    def _predict(group, node):\n",
    "        '''Recursively assign the input feature to one of the childs of \n",
    "        current node, until it finds a leaf node.\n",
    "        \n",
    "        Input:\n",
    "            group: list, input feature\n",
    "            node: Node object\n",
    "        Return:\n",
    "            terminal value of a leaf node\n",
    "        '''\n",
    "        if node.get_terminal() is not None:\n",
    "            return node.get_terminal()\n",
    "        index, value = node.get_ind_val()\n",
    "        if group[index] < value:\n",
    "            return CART_tree._predict(group, node.left)\n",
    "        else:\n",
    "            return CART_tree._predict(group, node.right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test this with a simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [[2.771244718,1.784783929,0],\n",
    "\t[1.728571309,1.169761413,0],\n",
    "\t[3.678319846,2.81281357,0],\n",
    "\t[3.961043357,2.61995032,0],\n",
    "\t[2.999208922,2.209014212,0],\n",
    "\t[7.497545867,3.162953546,1],\n",
    "\t[9.00220326,3.339047188,1],\n",
    "\t[7.444542326,0.476683375,1],\n",
    "\t[10.12493903,3.234550982,1],\n",
    "\t[6.642287351,3.319983761,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = CART_tree()\n",
    "temp.train(dataset, 1, 1)\n",
    "temp.predict(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More to do\n",
    "1. Use entropy to split nodes\n",
    "1. Tree pruning to reduce overfitting\n",
    "1. Categorical dataset, use equality instead of ranking\n",
    "1. Regression, use different cost function and method to create leaf nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
